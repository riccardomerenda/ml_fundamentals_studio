{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling and Engineering\n",
    "\n",
    "This notebook explores the importance of feature scaling and feature engineering in machine learning. We'll cover:\n",
    "\n",
    "1. Different scaling techniques (Min-Max, Standardization)\n",
    "2. The effect of feature scaling on gradient descent convergence\n",
    "3. Feature engineering techniques\n",
    "4. How these techniques affect model performance\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split as sk_train_test_split\n",
    "from sklearn.preprocessing import StandardScaler as SklearnStandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler as SklearnMinMaxScaler\n",
    "\n",
    "# Add the parent directory to sys.path to import our custom modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import our implementations\n",
    "from models.linear_regression import LinearRegression\n",
    "from utils.preprocessing import StandardScaler, MinMaxScaler\n",
    "from utils.plotting import plot_learning_curve\n",
    "from datasets.data_utils import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Data with Different Scales\n",
    "\n",
    "Let's first create some data with features on very different scales to demonstrate the importance of feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with features on different scales\n",
    "n_samples = 1000\n",
    "\n",
    "# Feature 1: Small range (0-1)\n",
    "X1 = np.random.rand(n_samples)\n",
    "\n",
    "# Feature 2: Medium range (0-100)\n",
    "X2 = np.random.rand(n_samples) * 100\n",
    "\n",
    "# Feature 3: Large range (0-10000)\n",
    "X3 = np.random.rand(n_samples) * 10000\n",
    "\n",
    "# Combine features\n",
    "X = np.column_stack((X1, X2, X3))\n",
    "\n",
    "# Generate target (with more influence from feature 1)\n",
    "y = 5 * X1 + 0.05 * X2 + 0.0005 * X3 + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Let's look at the ranges of each feature\n",
    "print(\"Feature ranges:\")\n",
    "for i in range(X.shape[1]):\n",
    "    print(f\"Feature {i+1}: Min = {X[:, i].min():.4f}, Max = {X[:, i].max():.4f}, Range = {X[:, i].max() - X[:, i].min():.4f}\")\n",
    "\n",
    "# Visualize feature distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(X[:, 0], bins=30, alpha=0.7)\n",
    "plt.title('Feature 1 Distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(X[:, 1], bins=30, alpha=0.7)\n",
    "plt.title('Feature 2 Distribution')\n",
    "plt.xlabel('Value')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(X[:, 2], bins=30, alpha=0.7)\n",
    "plt.title('Feature 3 Distribution')\n",
    "plt.xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing Feature Scaling\n",
    "\n",
    "Let's use our custom scalers to normalize the features and compare with scikit-learn's implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our min-max scaler\n",
    "our_mm_scaler = MinMaxScaler()\n",
    "X_train_mm_our = our_mm_scaler.fit_transform(X_train)\n",
    "X_test_mm_our = our_mm_scaler.transform(X_test)\n",
    "\n",
    "# Our standard scaler\n",
    "our_std_scaler = StandardScaler()\n",
    "X_train_std_our = our_std_scaler.fit_transform(X_train)\n",
    "X_test_std_our = our_std_scaler.transform(X_test)\n",
    "\n",
    "# scikit-learn's min-max scaler\n",
    "sk_mm_scaler = SklearnMinMaxScaler()\n",
    "X_train_mm_sk = sk_mm_scaler.fit_transform(X_train)\n",
    "X_test_mm_sk = sk_mm_scaler.transform(X_test)\n",
    "\n",
    "# scikit-learn's standard scaler\n",
    "sk_std_scaler = SklearnStandardScaler()\n",
    "X_train_std_sk = sk_std_scaler.fit_transform(X_train)\n",
    "X_test_std_sk = sk_std_scaler.transform(X_test)\n",
    "\n",
    "# Verify that our implementation matches sklearn's\n",
    "mm_diff = np.abs(X_train_mm_our - X_train_mm_sk).mean()\n",
    "std_diff = np.abs(X_train_std_our - X_train_std_sk).mean()\n",
    "\n",
    "print(f\"Average difference for MinMaxScaler: {mm_diff:.10f}\")\n",
    "print(f\"Average difference for StandardScaler: {std_diff:.10f}\")\n",
    "\n",
    "# Let's visualize the scaled features\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Original data\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.boxplot(X_train)\n",
    "plt.title('Original Features')\n",
    "plt.xticks([1, 2, 3], ['Feature 1', 'Feature 2', 'Feature 3'])\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Min-Max scaled (our implementation)\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.boxplot(X_train_mm_our)\n",
    "plt.title('Min-Max Scaled (Our Implementation)')\n",
    "plt.xticks([1, 2, 3], ['Feature 1', 'Feature 2', 'Feature 3'])\n",
    "\n",
    "# Standard scaled (our implementation)\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.boxplot(X_train_std_our)\n",
    "plt.title('Standard Scaled (Our Implementation)')\n",
    "plt.xticks([1, 2, 3], ['Feature 1', 'Feature 2', 'Feature 3'])\n",
    "\n",
    "# Min-Max scaled (scikit-learn)\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.boxplot(X_train_mm_sk)\n",
    "plt.title('Min-Max Scaled (scikit-learn)')\n",
    "plt.xticks([1, 2, 3], ['Feature 1', 'Feature 2', 'Feature 3'])\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Standard scaled (scikit-learn)\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.boxplot(X_train_std_sk)\n",
    "plt.title('Standard Scaled (scikit-learn)')\n",
    "plt.xticks([1, 2, 3], ['Feature 1', 'Feature 2', 'Feature 3'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Effect of Scaling on Gradient Descent\n",
    "\n",
    "Let's compare how gradient descent behaves with and without feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train model and track performance\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test, learning_rate=0.01, max_iterations=500, description=\"Model\"):\n",
    "    model = LinearRegression(learning_rate=learning_rate, max_iterations=max_iterations, store_history=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_mse = np.mean((model.predict(X_train) - y_train) ** 2)\n",
    "    test_mse = np.mean((model.predict(X_test) - y_test) ** 2)\n",
    "    \n",
    "    print(f\"{description}:\")\n",
    "    print(f\"  Weights: {model.weights}\")\n",
    "    print(f\"  Bias: {model.bias:.4f}\")\n",
    "    print(f\"  Training MSE: {train_mse:.4f}\")\n",
    "    print(f\"  Test MSE: {test_mse:.4f}\")\n",
    "    print(f\"  Iterations: {len(model.cost_history)}\")\n",
    "    \n",
    "    return model, train_mse, test_mse\n",
    "\n",
    "# Train models with different scaling\n",
    "print(\"Training models with different scaling approaches...\")\n",
    "\n",
    "# No scaling (may diverge or take a very long time)\n",
    "try:\n",
    "    model_no_scale, train_mse_no_scale, test_mse_no_scale = train_and_evaluate(\n",
    "        X_train, y_train, X_test, y_test, learning_rate=0.0000001, max_iterations=1000, \n",
    "        description=\"No scaling (very small learning rate)\")\n",
    "except Exception as e:\n",
    "    print(f\"No scaling model failed: {e}\")\n",
    "    model_no_scale = None\n",
    "\n",
    "# Min-Max scaling\n",
    "model_mm, train_mse_mm, test_mse_mm = train_and_evaluate(\n",
    "    X_train_mm_our, y_train, X_test_mm_our, y_test, learning_rate=0.1, max_iterations=1000, \n",
    "    description=\"Min-Max scaling\")\n",
    "\n",
    "# Standard scaling\n",
    "model_std, train_mse_std, test_mse_std = train_and_evaluate(\n",
    "    X_train_std_our, y_train, X_test_std_our, y_test, learning_rate=0.1, max_iterations=1000, \n",
    "    description=\"Standard scaling\")\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "if model_no_scale is not None:\n",
    "    plt.plot(model_no_scale.cost_history, label='No scaling')\n",
    "    \n",
    "plt.plot(model_mm.cost_history, label='Min-Max scaling')\n",
    "plt.plot(model_std.cost_history, label='Standard scaling')\n",
    "\n",
    "plt.title('Learning Curves with Different Scaling Methods')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering: Creating New Features\n",
    "\n",
    "Feature engineering involves creating new features from existing ones to improve model performance.\n",
    "Let's explore some common feature engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "california = fetch_california_housing()\n",
    "X_cal = california.data\n",
    "y_cal = california.target\n",
    "feature_names = california.feature_names\n",
    "\n",
    "print(\"California Housing Dataset:\")\n",
    "print(f\"Number of samples: {X_cal.shape[0]}\")\n",
    "print(f\"Number of features: {X_cal.shape[1]}\")\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "\n",
    "# Split data\n",
    "X_train_cal, X_test_cal, y_train_cal, y_test_cal = train_test_split(X_cal, y_cal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_cal_scaled = scaler.fit_transform(X_train_cal)\n",
    "X_test_cal_scaled = scaler.transform(X_test_cal)\n",
    "\n",
    "# Train baseline model\n",
    "model_baseline, train_mse_baseline, test_mse_baseline = train_and_evaluate(\n",
    "    X_train_cal_scaled, y_train_cal, X_test_cal_scaled, y_test_cal, \n",
    "    learning_rate=0.1, max_iterations=1000, description=\"Baseline model\")\n",
    "\n",
    "# Feature engineering techniques\n",
    "def add_polynomial_features(X, degree=2):\n",
    "    \"\"\"Add polynomial features up to the specified degree.\"\"\"\n",
    "    X_poly = X.copy()\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Add squared terms (x^2)\n",
    "    if degree >= 2:\n",
    "        for i in range(n_features):\n",
    "            X_poly = np.column_stack((X_poly, X[:, i]**2))\n",
    "    \n",
    "    # Add cubic terms (x^3)\n",
    "    if degree >= 3:\n",
    "        for i in range(n_features):\n",
    "            X_poly = np.column_stack((X_poly, X[:, i]**3))\n",
    "    \n",
    "    # Add interaction terms (x_i * x_j)\n",
    "    for i in range(n_features):\n",
    "        for j in range(i+1, n_features):\n",
    "            X_poly = np.column_stack((X_poly, X[:, i] * X[:, j]))\n",
    "    \n",
    "    return X_poly\n",
    "\n",
    "def add_log_features(X):\n",
    "    \"\"\"Add logarithmic transformations of positive features.\"\"\"\n",
    "    X_log = X.copy()\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        # Check if feature is positive\n",
    "        if np.all(X[:, i] > 0):\n",
    "            X_log = np.column_stack((X_log, np.log(X[:, i])))\n",
    "    \n",
    "    return X_log\n",
    "\n",
    "# Create new feature sets\n",
    "X_train_poly = add_polynomial_features(X_train_cal, degree=2)\n",
    "X_test_poly = add_polynomial_features(X_test_cal, degree=2)\n",
    "\n",
    "X_train_log = add_log_features(X_train_cal)\n",
    "X_test_log = add_log_features(X_test_cal)\n",
    "\n",
    "# Scale new feature sets\n",
    "scaler_poly = StandardScaler()\n",
    "X_train_poly_scaled = scaler_poly.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler_poly.transform(X_test_poly)\n",
    "\n",
    "scaler_log = StandardScaler()\n",
    "X_train_log_scaled = scaler_log.fit_transform(X_train_log)\n",
    "X_test_log_scaled = scaler_log.transform(X_test_log)\n",
    "\n",
    "# Train models with engineered features\n",
    "model_poly, train_mse_poly, test_mse_poly = train_and_evaluate(\n",
    "    X_train_poly_scaled, y_train_cal, X_test_poly_scaled, y_test_cal, \n",
    "    learning_rate=0.1, max_iterations=1000, description=\"Polynomial features model\")\n",
    "\n",
    "model_log, train_mse_log, test_mse_log = train_and_evaluate(\n",
    "    X_train_log_scaled, y_train_cal, X_test_log_scaled, y_test_cal, \n",
    "    learning_rate=0.1, max_iterations=1000, description=\"Log features model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Model Performance with Different Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MSE values\n",
    "models = ['Baseline', 'Polynomial Features', 'Log Features']\n",
    "train_mse_values = [train_mse_baseline, train_mse_poly, train_mse_log]\n",
    "test_mse_values = [test_mse_baseline, test_mse_poly, test_mse_log]\n",
    "\n",
    "# Plot MSE comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, train_mse_values, width, label='Training MSE')\n",
    "plt.bar(x + width/2, test_mse_values, width, label='Test MSE')\n",
    "\n",
    "plt.xlabel('Model Type')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('MSE Comparison with Different Feature Engineering Techniques')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Display feature counts\n",
    "print(\"Feature count comparison:\")\n",
    "print(f\"Original features: {X_train_cal.shape[1]}\")\n",
    "print(f\"With polynomial features: {X_train_poly.shape[1]}\")\n",
    "print(f\"With log features: {X_train_log.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance and Selection\n",
    "\n",
    "Let's analyze which features are most important for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display baseline model weights\n",
    "baseline_weights = np.abs(model_baseline.weights)\n",
    "feature_importance = baseline_weights / np.sum(baseline_weights)\n",
    "\n",
    "# Create a dataframe with feature names and importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance,\n",
    "    'Weight': model_baseline.weights\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Display feature importance\n",
    "print(\"Feature importance for baseline model:\")\n",
    "display(importance_df)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance for California Housing Price Prediction')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Feature Relationships\n",
    "\n",
    "Let's create scatter plots to visualize the relationship between important features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 4 important features\n",
    "top_features = importance_df['Feature'].values[:4]\n",
    "# top_indices = [feature_names.tolist().index(feature) for feature in top_features]\n",
    "top_indices = [feature_names.index(feature) for feature in top_features]\n",
    "\n",
    "# Create scatter plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, (feature, idx) in enumerate(zip(top_features, top_indices)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.scatter(X_cal[:, idx], y_cal, alpha=0.5, s=10)\n",
    "    \n",
    "    # Add linear regression line for visual reference\n",
    "    z = np.polyfit(X_cal[:, idx], y_cal, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_range = np.linspace(X_cal[:, idx].min(), X_cal[:, idx].max(), 100)\n",
    "    plt.plot(x_range, p(x_range), 'r--')\n",
    "    \n",
    "    plt.title(f'Relationship of {feature} with House Prices')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('House Price')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Combining Feature Scaling and Engineering\n",
    "\n",
    "Let's build a model that uses both feature scaling and feature engineering to get the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for our best feature engineering approach\n",
    "def engineer_features(X):\n",
    "    \"\"\"Apply our best feature engineering approach based on previous results.\"\"\"\n",
    "    # Start with original features\n",
    "    X_engineered = X.copy()\n",
    "    \n",
    "    # Add polynomial features for the most important features\n",
    "    for idx in top_indices[:2]:  # Only use top 2 features for simplicity\n",
    "        X_engineered = np.column_stack((X_engineered, X[:, idx]**2))\n",
    "    \n",
    "    # Add interaction between the top 2 features\n",
    "    X_engineered = np.column_stack((X_engineered, X[:, top_indices[0]] * X[:, top_indices[1]]))\n",
    "    \n",
    "    # Add log transformations for positive features\n",
    "    for i in range(X.shape[1]):\n",
    "        if np.all(X[:, i] > 0):\n",
    "            X_engineered = np.column_stack((X_engineered, np.log(X[:, i])))\n",
    "    \n",
    "    return X_engineered\n",
    "\n",
    "# Apply our engineering function\n",
    "X_train_best = engineer_features(X_train_cal)\n",
    "X_test_best = engineer_features(X_test_cal)\n",
    "\n",
    "# Scale the features\n",
    "scaler_best = StandardScaler()\n",
    "X_train_best_scaled = scaler_best.fit_transform(X_train_best)\n",
    "X_test_best_scaled = scaler_best.transform(X_test_best)\n",
    "\n",
    "# Train the model\n",
    "model_best, train_mse_best, test_mse_best = train_and_evaluate(\n",
    "    X_train_best_scaled, y_train_cal, X_test_best_scaled, y_test_cal, \n",
    "    learning_rate=0.1, max_iterations=1000, description=\"Best engineered model\")\n",
    "\n",
    "# Compare all models\n",
    "models = ['Baseline', 'Polynomial Features', 'Log Features', 'Best Engineered']\n",
    "train_mse_values = [train_mse_baseline, train_mse_poly, train_mse_log, train_mse_best]\n",
    "test_mse_values = [test_mse_baseline, test_mse_poly, test_mse_log, test_mse_best]\n",
    "\n",
    "# Plot final comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, train_mse_values, width, label='Training MSE')\n",
    "plt.bar(x + width/2, test_mse_values, width, label='Test MSE')\n",
    "\n",
    "plt.xlabel('Model Type')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Final MSE Comparison with Different Feature Engineering Techniques')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary of Key Findings\n",
    "\n",
    "In this notebook, we've explored the importance of feature scaling and engineering:\n",
    "\n",
    "1. **Feature Scaling Findings**:\n",
    "   - Without scaling, gradient descent struggles to converge when features have very different scales\n",
    "   - Both MinMax and Standardization scaling methods greatly improve convergence\n",
    "   - Proper scaling allows us to use a larger learning rate, making training faster\n",
    "\n",
    "2. **Feature Engineering Findings**:\n",
    "   - Adding polynomial terms can capture non-linear relationships\n",
    "   - Log transformations can be useful for features with skewed distributions\n",
    "   - Feature interaction terms can capture relationships between features\n",
    "   - Engineered features can significantly improve model performance\n",
    "\n",
    "3. **Important Principles**:\n",
    "   - Always scale features before applying gradient descent\n",
    "   - Understand your data and feature relationships before engineering new features\n",
    "   - Not all engineered features will be useful; evaluate their impact on model performance\n",
    "   - Combining the right scaling and feature engineering techniques leads to the best results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
