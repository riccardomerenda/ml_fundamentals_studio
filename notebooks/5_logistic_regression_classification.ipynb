{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification\n",
    "\n",
    "This notebook explores binary classification using logistic regression. We'll implement and analyze logistic regression for classification tasks, visualize decision boundaries, and understand how regularization applies to classification problems.\n",
    "\n",
    "Topics covered:\n",
    "1. Understanding the logistic regression model\n",
    "2. Sigmoid function and classification probability\n",
    "3. Cross-entropy loss function\n",
    "4. Training and evaluating a logistic regression classifier\n",
    "5. Visualizing decision boundaries\n",
    "6. Impact of regularization on classification\n",
    "7. Handling non-linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Add the parent directory to sys.path to import our custom modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import our implementations\n",
    "from models.logistic_regression import LogisticRegression\n",
    "from utils.preprocessing import StandardScaler as OurStandardScaler\n",
    "from utils.plotting import plot_decision_boundary, plot_learning_curve\n",
    "from utils.metrics import confusion_matrix as our_confusion_matrix\n",
    "from utils.metrics import precision_recall_f1\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Logistic Regression\n",
    "\n",
    "Unlike linear regression, which predicts continuous values, logistic regression predicts the probability that an instance belongs to a particular class. It uses the sigmoid function to transform the linear prediction into a probability value between 0 and 1.\n",
    "\n",
    "The key components are:\n",
    "1. **Linear combination**: $z = w^T x + b$\n",
    "2. **Sigmoid function**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "3. **Prediction**: $\\hat{y} = \\sigma(z)$\n",
    "4. **Decision rule**: Predict class 1 if $\\hat{y} \\geq 0.5$, otherwise class 0\n",
    "\n",
    "Let's visualize the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid of z.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Plot the sigmoid function\n",
    "z = np.linspace(-10, 10, 100)\n",
    "y = sigmoid(z)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, y, 'b-', linewidth=2)\n",
    "plt.axhline(y=0.5, color='k', linestyle='--', label='y = 0.5')\n",
    "plt.axvline(x=0, color='r', linestyle='--', label='z = 0')\n",
    "plt.grid(True)\n",
    "plt.xlabel('z = w^T x + b')\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"The sigmoid function maps any real number to the range (0, 1).\")\n",
    "print(\"When z = 0, sigmoid(z) = 0.5\")\n",
    "print(\"As z approaches positive infinity, sigmoid(z) approaches 1.\")\n",
    "print(\"As z approaches negative infinity, sigmoid(z) approaches 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a Classification Dataset\n",
    "\n",
    "Let's generate a simple classification dataset to demonstrate logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a binary classification dataset\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=42, n_clusters_per_class=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Plot the dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], marker='o', label='Class 0 (Training)', alpha=0.7)\n",
    "plt.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], marker='^', label='Class 1 (Training)', alpha=0.7)\n",
    "plt.scatter(X_test[y_test == 0][:, 0], X_test[y_test == 0][:, 1], marker='o', label='Class 0 (Test)', alpha=0.7, facecolors='none', edgecolors='blue')\n",
    "plt.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], marker='^', label='Class 1 (Test)', alpha=0.7, facecolors='none', edgecolors='orange')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Binary Classification Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a Logistic Regression Model\n",
    "\n",
    "Now, let's train our custom logistic regression model on this dataset and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our logistic regression model\n",
    "lr_model = LogisticRegression(learning_rate=0.1, max_iterations=1000, regularization=0.0)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = lr_model.predict(X_train_scaled)\n",
    "y_test_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Display the model weights\n",
    "print(\"\\nModel weights:\")\n",
    "print(f\"w = {lr_model.weights}\")\n",
    "print(f\"b = {lr_model.bias:.4f}\")\n",
    "\n",
    "# Compute and display confusion matrix\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm = our_confusion_matrix(y_test, y_test_pred)\n",
    "print(cm)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1 = precision_recall_f1(y_test, y_test_pred)\n",
    "print(f\"\\nPrecision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing the Decision Boundary\n",
    "\n",
    "Let's visualize the decision boundary of our logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "fig = plot_decision_boundary(lr_model, X_train_scaled, y_train, \n",
    "                           title=\"Logistic Regression Decision Boundary (Training Set)\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision boundary for the test set\n",
    "fig = plot_decision_boundary(lr_model, X_test_scaled, y_test, \n",
    "                           title=\"Logistic Regression Decision Boundary (Test Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examining Predicted Probabilities\n",
    "\n",
    "One of the advantages of logistic regression is that it provides probabilities, not just class predictions. Let's examine these probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities\n",
    "probs_train = lr_model.predict_proba(X_train_scaled)\n",
    "probs_test = lr_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Plot histograms of predicted probabilities\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(probs_train[y_train == 0], alpha=0.5, label='Class 0', bins=20, range=(0, 1))\n",
    "plt.hist(probs_train[y_train == 1], alpha=0.5, label='Class 1', bins=20, range=(0, 1))\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Decision boundary')\n",
    "plt.xlabel('Predicted probability of Class 1')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Probabilities (Training Set)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(probs_test[y_test == 0], alpha=0.5, label='Class 0', bins=20, range=(0, 1))\n",
    "plt.hist(probs_test[y_test == 1], alpha=0.5, label='Class 1', bins=20, range=(0, 1))\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Decision boundary')\n",
    "plt.xlabel('Predicted probability of Class 1')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Probabilities (Test Set)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Effect of Regularization\n",
    "\n",
    "Let's explore how regularization affects the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with different regularization strengths\n",
    "lambda_values = [0, 0.01, 0.1, 1.0, 10.0]\n",
    "models_reg = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for lambda_val in lambda_values:\n",
    "    model = LogisticRegression(learning_rate=0.1, max_iterations=1000, regularization=lambda_val)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    models_reg.append(model)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    train_pred = model.predict(X_train_scaled)\n",
    "    test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    train_acc.append(accuracy_score(y_train, train_pred))\n",
    "    test_acc.append(accuracy_score(y_test, test_pred))\n",
    "    \n",
    "    print(f\"λ = {lambda_val}: Train accuracy = {train_acc[-1]:.4f}, Test accuracy = {test_acc[-1]:.4f}\")\n",
    "\n",
    "# Plot accuracy vs. regularization strength\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(lambda_values, train_acc, 'b-o', linewidth=2, label='Training accuracy')\n",
    "plt.semilogx(lambda_values, test_acc, 'r-o', linewidth=2, label='Test accuracy')\n",
    "plt.xlabel('Regularization parameter (λ)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of Regularization on Logistic Regression Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Decision Boundaries with Different Regularization\n",
    "\n",
    "Let's visualize how regularization affects the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of models for visualization\n",
    "lambda_selected = [0, 0.1, 10.0]  # No, moderate, and strong regularization\n",
    "selected_indices = [lambda_values.index(lambda_val) for lambda_val in lambda_selected]\n",
    "models_selected = [models_reg[i] for i in selected_indices]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, (lambda_val, model) in enumerate(zip(lambda_selected, models_selected)):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    fig = plot_decision_boundary(model, X_train_scaled, y_train, step=0.05, \n",
    "                               title=f\"Decision Boundary (λ = {lambda_val})\")\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Handling Non-Linearly Separable Data\n",
    "\n",
    "Basic logistic regression works well for linearly separable data. Let's see how it performs on a non-linearly separable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a non-linearly separable dataset (moons)\n",
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train_moons, X_test_moons, y_train_moons, y_test_moons = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_moons = StandardScaler()\n",
    "X_train_moons_scaled = scaler_moons.fit_transform(X_train_moons)\n",
    "X_test_moons_scaled = scaler_moons.transform(X_test_moons)\n",
    "\n",
    "# Plot the dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], marker='o', label='Class 0', alpha=0.7)\n",
    "plt.scatter(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], marker='^', label='Class 1', alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Non-Linearly Separable Dataset (Moons)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Train a logistic regression model on the moons dataset\n",
    "lr_moons = LogisticRegression(learning_rate=0.1, max_iterations=1000, regularization=0.1)\n",
    "lr_moons.fit(X_train_moons_scaled, y_train_moons)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_moons = lr_moons.predict(X_train_moons_scaled)\n",
    "y_test_pred_moons = lr_moons.predict(X_test_moons_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy_moons = accuracy_score(y_train_moons, y_train_pred_moons)\n",
    "test_accuracy_moons = accuracy_score(y_test_moons, y_test_pred_moons)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy_moons:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy_moons:.4f}\")\n",
    "\n",
    "# Plot the decision boundary\n",
    "fig = plot_decision_boundary(lr_moons, X_train_moons_scaled, y_train_moons, \n",
    "                           title=\"Logistic Regression on Non-Linearly Separable Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Adding Polynomial Features for Non-Linear Decision Boundaries\n",
    "\n",
    "We can handle non-linearly separable data by adding polynomial features, similar to what we did for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polynomial_features(X, degree=2):\n",
    "    \"\"\"Add polynomial features up to the specified degree.\"\"\"\n",
    "    X_poly = X.copy()\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Add squared terms (x^2)\n",
    "    if degree >= 2:\n",
    "        for i in range(n_features):\n",
    "            X_poly = np.column_stack((X_poly, X[:, i]**2))\n",
    "    \n",
    "    # Add interaction terms (x_i * x_j)\n",
    "    for i in range(n_features):\n",
    "        for j in range(i+1, n_features):\n",
    "            X_poly = np.column_stack((X_poly, X[:, i] * X[:, j]))\n",
    "    \n",
    "    # Add cubic terms (x^3)\n",
    "    if degree >= 3:\n",
    "        for i in range(n_features):\n",
    "            X_poly = np.column_stack((X_poly, X[:, i]**3))\n",
    "    \n",
    "    return X_poly\n",
    "\n",
    "# Add polynomial features to the moons dataset\n",
    "X_train_moons_poly = add_polynomial_features(X_train_moons_scaled, degree=3)\n",
    "X_test_moons_poly = add_polynomial_features(X_test_moons_scaled, degree=3)\n",
    "\n",
    "print(f\"Original number of features: {X_train_moons_scaled.shape[1]}\")\n",
    "print(f\"Number of features after polynomial expansion: {X_train_moons_poly.shape[1]}\")\n",
    "\n",
    "# Train logistic regression on the expanded features\n",
    "lr_moons_poly = LogisticRegression(learning_rate=0.1, max_iterations=1000, regularization=0.1)\n",
    "lr_moons_poly.fit(X_train_moons_poly, y_train_moons)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_moons_poly = lr_moons_poly.predict(X_train_moons_poly)\n",
    "y_test_pred_moons_poly = lr_moons_poly.predict(X_test_moons_poly)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy_moons_poly = accuracy_score(y_train_moons, y_train_pred_moons_poly)\n",
    "test_accuracy_moons_poly = accuracy_score(y_test_moons, y_test_pred_moons_poly)\n",
    "\n",
    "print(f\"\\nPolynomial Features:\")\n",
    "print(f\"Training accuracy: {train_accuracy_moons_poly:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy_moons_poly:.4f}\")\n",
    "print(f\"Improvement over linear model: {(test_accuracy_moons_poly - test_accuracy_moons)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can't directly visualize the decision boundary in the expanded feature space, let's create a mesh grid in the original 2D space and see how the model classifies each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a meshgrid in the original feature space\n",
    "h = 0.02  # step size in the mesh\n",
    "x_min, x_max = X_moons[:, 0].min() - 1, X_moons[:, 0].max() + 1\n",
    "y_min, y_max = X_moons[:, 1].min() - 1, X_moons[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Create a grid of points\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Scale the grid points\n",
    "grid_scaled = scaler_moons.transform(grid)\n",
    "\n",
    "# Add polynomial features to the grid\n",
    "grid_poly = add_polynomial_features(grid_scaled, degree=3)\n",
    "\n",
    "# Predict class for each point in the grid\n",
    "Z_linear = lr_moons.predict(grid_scaled)\n",
    "Z_poly = lr_moons_poly.predict(grid_poly)\n",
    "\n",
    "# Reshape results to match the meshgrid shape\n",
    "Z_linear = Z_linear.reshape(xx.shape)\n",
    "Z_poly = Z_poly.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot linear decision boundary\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(xx, yy, Z_linear, alpha=0.8, cmap=plt.cm.RdBu)\n",
    "plt.scatter(X_train_moons[:, 0], X_train_moons[:, 1], c=y_train_moons, cmap=plt.cm.RdBu, edgecolors='k')\n",
    "plt.title(f\"Linear Boundary (Accuracy: {test_accuracy_moons:.4f})\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plot polynomial decision boundary\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contourf(xx, yy, Z_poly, alpha=0.8, cmap=plt.cm.RdBu)\n",
    "plt.scatter(X_train_moons[:, 0], X_train_moons[:, 1], c=y_train_moons, cmap=plt.cm.RdBu, edgecolors='k')\n",
    "plt.title(f\"Polynomial Boundary (Accuracy: {test_accuracy_moons_poly:.4f})\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison with Scikit-learn's Implementation\n",
    "\n",
    "Let's compare our implementation with scikit-learn's LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train scikit-learn model on the original dataset\n",
    "sk_lr = SklearnLogisticRegression(C=1/0.1, max_iter=1000, random_state=42)\n",
    "sk_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "sk_y_train_pred = sk_lr.predict(X_train_scaled)\n",
    "sk_y_test_pred = sk_lr.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "sk_train_accuracy = accuracy_score(y_train, sk_y_train_pred)\n",
    "sk_test_accuracy = accuracy_score(y_test, sk_y_test_pred)\n",
    "\n",
    "print(\"Comparison with scikit-learn:\")\n",
    "print(f\"Our model - Train accuracy: {train_acc[1]:.4f}, Test accuracy: {test_acc[1]:.4f}\")\n",
    "print(f\"Scikit-learn - Train accuracy: {sk_train_accuracy:.4f}, Test accuracy: {sk_test_accuracy:.4f}\")\n",
    "\n",
    "# Compare weights\n",
    "print(\"\\nModel weights:\")\n",
    "print(f\"Our model - w: {models_reg[1].weights}, b: {models_reg[1].bias:.4f}\")\n",
    "print(f\"Scikit-learn - w: {sk_lr.coef_[0]}, b: {sk_lr.intercept_[0]:.4f}\")\n",
    "\n",
    "# Plot decision boundaries side by side\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Our model\n",
    "plt.subplot(1, 2, 1)\n",
    "fig = plot_decision_boundary(models_reg[1], X_train_scaled, y_train, \n",
    "                           title=\"Our Implementation\")\n",
    "\n",
    "# Scikit-learn model\n",
    "plt.subplot(1, 2, 2)\n",
    "h = 0.02  # step size in the mesh\n",
    "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = sk_lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdBu)\n",
    "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, cmap=plt.cm.RdBu, edgecolors='k')\n",
    "plt.title(\"Scikit-learn Implementation\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary of Logistic Regression\n",
    "\n",
    "Let's summarize what we've learned about logistic regression for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts in Logistic Regression\n",
    "\n",
    "1. **Core Model Structure**\n",
    "   - Linear combination of features: $z = w^T x + b$\n",
    "   - Sigmoid function transforms $z$ to probability: $\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "   - Decision rule: Predict class 1 if $\\hat{y} \\geq 0.5$, otherwise class 0\n",
    "   - This is equivalent to: Predict class 1 if $z \\geq 0$, otherwise class 0\n",
    "\n",
    "2. **Training Process**\n",
    "   - Cross-entropy loss function measures how well predictions match true labels\n",
    "   - Gradient descent optimizes weights to minimize the loss\n",
    "   - Regularization helps prevent overfitting\n",
    "\n",
    "3. **Decision Boundary**\n",
    "   - In logistic regression, the decision boundary is a line/hyperplane: $w^T x + b = 0$\n",
    "   - The model predicts class 1 on one side and class 0 on the other\n",
    "   - The distance from the boundary relates to prediction confidence\n",
    "\n",
    "4. **Strengths of Logistic Regression**\n",
    "   - Provides probabilities, not just class predictions\n",
    "   - Simple and interpretable model with efficient training\n",
    "   - Works well for linearly separable data\n",
    "   - Regularization effectively controls overfitting\n",
    "\n",
    "5. **Limitations**\n",
    "   - Linear decision boundary can't separate non-linear data\n",
    "   - May underfit complex relationships in the data\n",
    "   - Struggles with imbalanced classes (not demonstrated here)\n",
    "\n",
    "6. **Addressing Non-Linearity**\n",
    "   - Polynomial features can create non-linear decision boundaries\n",
    "   - This maintains the model's simplicity while increasing flexibility\n",
    "   - But requires careful regularization to prevent overfitting with higher-order polynomials\n",
    "\n",
    "7. **Practical Tips**\n",
    "   - Always scale features for faster convergence and better results\n",
    "   - Use regularization to improve generalization\n",
    "   - Consider polynomial features for non-linearly separable data\n",
    "   - Examine predicted probabilities, not just class predictions\n",
    "   - Use metrics beyond accuracy (precision, recall, F1) for imbalanced problems\n",
    "\n",
    "### Comparison with Linear Regression\n",
    "\n",
    "1. **Output interpretation**\n",
    "   - Linear regression: Continuous values (potentially unrestricted range)\n",
    "   - Logistic regression: Probabilities between 0 and 1\n",
    "\n",
    "2. **Cost function**\n",
    "   - Linear regression: Mean Squared Error\n",
    "   - Logistic regression: Cross-Entropy Loss (Log Loss)\n",
    "\n",
    "3. **Application**\n",
    "   - Linear regression: Regression problems (predicting quantities)\n",
    "   - Logistic regression: Classification problems (predicting categories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
