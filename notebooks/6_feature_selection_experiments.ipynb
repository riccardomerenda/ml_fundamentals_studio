{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection: Methods and Impact on Model Performance\n",
    "\n",
    "This notebook explores various feature selection techniques and their effects on machine learning model performance. We'll investigate how selecting the right subset of features can improve model accuracy, reduce overfitting, and enhance interpretability.\n",
    "\n",
    "## What is Feature Selection?\n",
    "\n",
    "Feature selection is the process of identifying and selecting a subset of input features that are most relevant to the predictive modeling problem. It helps to:\n",
    "\n",
    "- **Reduce dimensionality** and simplify models\n",
    "- **Improve performance** by removing irrelevant or redundant features\n",
    "- **Reduce overfitting** and enhance generalization\n",
    "- **Increase interpretability** by focusing on the most important variables\n",
    "- **Decrease computational cost** during model training and inference\n",
    "\n",
    "Let's explore several feature selection approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from sklearn.datasets import load_diabetes, fetch_california_housing  # Changed this line\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import our implementations\n",
    "from models.linear_regression import LinearRegression\n",
    "from models.logistic_regression import LogisticRegression\n",
    "from utils.feature_selection import FeatureSelector, plot_feature_importance\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a Synthetic Dataset with Known Important Features\n",
    "\n",
    "To properly evaluate feature selection techniques, we'll start by creating a synthetic dataset where we know exactly which features are important and which are irrelevant noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_data(n_samples=500, n_features=50, n_informative=10, noise=0.5):\n",
    "    \"\"\"Create a synthetic regression dataset with known important features.\"\"\"\n",
    "    # Generate random feature matrix\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Generate weights: only n_informative features have non-zero weights\n",
    "    true_weights = np.zeros(n_features)\n",
    "    informative_indices = np.random.choice(n_features, n_informative, replace=False)\n",
    "    true_weights[informative_indices] = np.random.uniform(1, 5, size=n_informative)\n",
    "    \n",
    "    # Generate target variable\n",
    "    y = np.dot(X, true_weights) + np.random.normal(0, noise, size=n_samples)\n",
    "    \n",
    "    return X, y, true_weights, informative_indices\n",
    "\n",
    "# Create dataset\n",
    "n_features = 50\n",
    "n_informative = 10\n",
    "X, y, true_weights, informative_indices = create_synthetic_data(\n",
    "    n_samples=500, n_features=n_features, n_informative=n_informative, noise=1.0)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Visualize the true feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(n_features), np.abs(true_weights), color='skyblue')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Absolute Weight')\n",
    "plt.title('True Feature Importance')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Highlight informative features\n",
    "for idx in informative_indices:\n",
    "    plt.bar(idx, np.abs(true_weights[idx]), color='tomato')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset created with {n_features} total features\")\n",
    "print(f\"Only {n_informative} features are truly informative: {informative_indices}\")\n",
    "print(f\"The remaining {n_features - n_informative} features are noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluating a Model with All Features (Baseline)\n",
    "\n",
    "First, let's establish a baseline by training a model using all features, including the irrelevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for better performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model with all features\n",
    "model_all = LinearRegression(learning_rate=0.01, max_iterations=2000, regularization=0.0)\n",
    "model_all.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "y_pred_train = model_all.predict(X_train_scaled)\n",
    "y_pred_test = model_all.predict(X_test_scaled)\n",
    "\n",
    "mse_train_all = mean_squared_error(y_train, y_pred_train)\n",
    "mse_test_all = mean_squared_error(y_test, y_pred_test)\n",
    "r2_test_all = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Baseline Model (All {n_features} Features)\")\n",
    "print(f\"Training MSE: {mse_train_all:.4f}\")\n",
    "print(f\"Test MSE: {mse_test_all:.4f}\")\n",
    "print(f\"Test R²: {r2_test_all:.4f}\")\n",
    "\n",
    "# Compare learned weights with true weights\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(n_features), np.abs(true_weights), alpha=0.5, label='True Weights')\n",
    "plt.bar(range(n_features), np.abs(model_all.weights), alpha=0.5, label='Learned Weights')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Absolute Weight')\n",
    "plt.title('True vs. Learned Feature Weights')\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Calculate how many of the top features match the true informative features\n",
    "top_n_indices = np.argsort(np.abs(model_all.weights))[-n_informative:]\n",
    "matches = np.intersect1d(top_n_indices, informative_indices)\n",
    "print(f\"\\nOf the top {n_informative} learned features, {len(matches)} match the true informative features.\")\n",
    "print(f\"Match rate: {len(matches)/n_informative:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter Methods: Correlation-Based Feature Selection\n",
    "\n",
    "Filter methods select features based on statistical measures. The simplest approach is to select features with the highest correlation with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature selector\n",
    "selector = FeatureSelector()\n",
    "\n",
    "# Apply correlation-based selection to choose top k features\n",
    "k = n_informative  # Select the same number as true informative features\n",
    "selected_features_corr, corr_scores = selector.select_k_best(X_train_scaled, y_train, k=k)\n",
    "\n",
    "# Plot correlation scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(n_features), corr_scores, color='lightblue')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Absolute Correlation')\n",
    "plt.title('Feature Correlation with Target')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Highlight selected features\n",
    "for idx in selected_features_corr:\n",
    "    plt.bar(idx, corr_scores[idx], color='orange')\n",
    "    \n",
    "# Highlight true informative features with a different marker\n",
    "for idx in informative_indices:\n",
    "    plt.axvline(x=idx, color='red', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate overlap with true informative features\n",
    "overlap_corr = np.intersect1d(selected_features_corr, informative_indices)\n",
    "print(f\"Correlation-based feature selection:\")\n",
    "print(f\"Selected {k} features: {selected_features_corr}\")\n",
    "print(f\"Correctly identified {len(overlap_corr)} out of {n_informative} informative features\")\n",
    "print(f\"Accuracy: {len(overlap_corr)/n_informative:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model using only correlation-selected features\n",
    "X_train_corr = X_train_scaled[:, selected_features_corr]\n",
    "X_test_corr = X_test_scaled[:, selected_features_corr]\n",
    "\n",
    "model_corr = LinearRegression(learning_rate=0.01, max_iterations=2000, regularization=0.0)\n",
    "model_corr.fit(X_train_corr, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "y_pred_train_corr = model_corr.predict(X_train_corr)\n",
    "y_pred_test_corr = model_corr.predict(X_test_corr)\n",
    "\n",
    "mse_train_corr = mean_squared_error(y_train, y_pred_train_corr)\n",
    "mse_test_corr = mean_squared_error(y_test, y_pred_test_corr)\n",
    "r2_test_corr = r2_score(y_test, y_pred_test_corr)\n",
    "\n",
    "print(f\"Correlation-Based Feature Selection Model ({k} Features)\")\n",
    "print(f\"Training MSE: {mse_train_corr:.4f}\")\n",
    "print(f\"Test MSE: {mse_test_corr:.4f}\")\n",
    "print(f\"Test R²: {r2_test_corr:.4f}\")\n",
    "print(f\"\\nImprovement over baseline (Test MSE): {(mse_test_all - mse_test_corr)/mse_test_all:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wrapper Methods: Recursive Feature Elimination (RFE)\n",
    "\n",
    "Wrapper methods use the model itself to evaluate feature importance. Recursive Feature Elimination (RFE) recursively removes the least important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply recursive feature elimination\n",
    "selected_features_rfe, feature_ranks = selector.recursive_feature_elimination(\n",
    "    X_train_scaled, y_train, n_features_to_select=k)\n",
    "\n",
    "# Plot feature ranks (lower rank = more important)\n",
    "importance_scores = n_features + 1 - feature_ranks  # Convert ranks to scores for visualization\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(n_features), importance_scores, color='lightgreen')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('RFE Feature Importance')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Highlight selected features\n",
    "for idx in selected_features_rfe:\n",
    "    plt.bar(idx, importance_scores[idx], color='green')\n",
    "    \n",
    "# Highlight true informative features with a different marker\n",
    "for idx in informative_indices:\n",
    "    plt.axvline(x=idx, color='red', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate overlap with true informative features\n",
    "overlap_rfe = np.intersect1d(selected_features_rfe, informative_indices)\n",
    "print(f\"Recursive Feature Elimination:\")\n",
    "print(f\"Selected {k} features: {selected_features_rfe}\")\n",
    "print(f\"Correctly identified {len(overlap_rfe)} out of {n_informative} informative features\")\n",
    "print(f\"Accuracy: {len(overlap_rfe)/n_informative:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model using only RFE-selected features\n",
    "X_train_rfe = X_train_scaled[:, selected_features_rfe]\n",
    "X_test_rfe = X_test_scaled[:, selected_features_rfe]\n",
    "\n",
    "model_rfe = LinearRegression(learning_rate=0.01, max_iterations=2000, regularization=0.0)\n",
    "model_rfe.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "y_pred_train_rfe = model_rfe.predict(X_train_rfe)\n",
    "y_pred_test_rfe = model_rfe.predict(X_test_rfe)\n",
    "\n",
    "mse_train_rfe = mean_squared_error(y_train, y_pred_train_rfe)\n",
    "mse_test_rfe = mean_squared_error(y_test, y_pred_test_rfe)\n",
    "r2_test_rfe = r2_score(y_test, y_pred_test_rfe)\n",
    "\n",
    "print(f\"RFE Feature Selection Model ({k} Features)\")\n",
    "print(f\"Training MSE: {mse_train_rfe:.4f}\")\n",
    "print(f\"Test MSE: {mse_test_rfe:.4f}\")\n",
    "print(f\"Test R²: {r2_test_rfe:.4f}\")\n",
    "print(f\"\\nImprovement over baseline (Test MSE): {(mse_test_all - mse_test_rfe)/mse_test_all:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedded Methods: L1 Regularization (Lasso) for Feature Selection\n",
    "\n",
    "Embedded methods perform feature selection during model training. L1 regularization (Lasso) can effectively drive irrelevant feature weights to exactly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply L1-based feature selection\n",
    "selected_features_l1, l1_importance = selector.l1_based_selection(\n",
    "    X_train_scaled, y_train, alpha=0.1, threshold=0.01)\n",
    "\n",
    "# Sort selected features for better display\n",
    "selected_features_l1 = np.sort(selected_features_l1)\n",
    "\n",
    "# Plot L1 feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(n_features), l1_importance, color='lightpink')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Absolute Weight')\n",
    "plt.title('L1 Regularization Feature Importance')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Highlight selected features\n",
    "for idx in selected_features_l1:\n",
    "    plt.bar(idx, l1_importance[idx], color='red')\n",
    "    \n",
    "# Highlight true informative features with a different marker\n",
    "for idx in informative_indices:\n",
    "    plt.axvline(x=idx, color='blue', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate overlap with true informative features\n",
    "overlap_l1 = np.intersect1d(selected_features_l1, informative_indices)\n",
    "print(f\"L1 Regularization Feature Selection:\")\n",
    "print(f\"Selected {len(selected_features_l1)} features: {selected_features_l1}\")\n",
    "print(f\"Correctly identified {len(overlap_l1)} out of {n_informative} informative features\")\n",
    "print(f\"Accuracy: {len(overlap_l1)/n_informative:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model using only L1-selected features\n",
    "X_train_l1 = X_train_scaled[:, selected_features_l1]\n",
    "X_test_l1 = X_test_scaled[:, selected_features_l1]\n",
    "\n",
    "model_l1 = LinearRegression(learning_rate=0.01, max_iterations=2000, regularization=0.0)\n",
    "model_l1.fit(X_train_l1, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "y_pred_train_l1 = model_l1.predict(X_train_l1)\n",
    "y_pred_test_l1 = model_l1.predict(X_test_l1)\n",
    "\n",
    "mse_train_l1 = mean_squared_error(y_train, y_pred_train_l1)\n",
    "mse_test_l1 = mean_squared_error(y_test, y_pred_test_l1)\n",
    "r2_test_l1 = r2_score(y_test, y_pred_test_l1)\n",
    "\n",
    "print(f\"L1-Based Feature Selection Model ({len(selected_features_l1)} Features)\")\n",
    "print(f\"Training MSE: {mse_train_l1:.4f}\")\n",
    "print(f\"Test MSE: {mse_test_l1:.4f}\")\n",
    "print(f\"Test R²: {r2_test_l1:.4f}\")\n",
    "print(f\"\\nImprovement over baseline (Test MSE): {(mse_test_all - mse_test_l1)/mse_test_all:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing All Feature Selection Methods\n",
    "\n",
    "Now let's compare the performance of all the feature selection methods we've explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to compare all methods\n",
    "comparison = pd.DataFrame({\n",
    "    'Method': ['All Features', 'Correlation-Based', 'RFE', 'L1 Regularization'],\n",
    "    'Feature Count': [n_features, len(selected_features_corr), len(selected_features_rfe), len(selected_features_l1)],\n",
    "    'Training MSE': [mse_train_all, mse_train_corr, mse_train_rfe, mse_train_l1],\n",
    "    'Test MSE': [mse_test_all, mse_test_corr, mse_test_rfe, mse_test_l1],\n",
    "    'Test R²': [r2_test_all, r2_test_corr, r2_test_rfe, r2_test_l1],\n",
    "    'Identified Correctly': [len(matches), len(overlap_corr), len(overlap_rfe), len(overlap_l1)],\n",
    "    'Identification Rate': [len(matches)/n_informative, len(overlap_corr)/n_informative, \n",
    "                          len(overlap_rfe)/n_informative, len(overlap_l1)/n_informative],\n",
    "})\n",
    "\n",
    "# Calculate improvement over baseline\n",
    "comparison['MSE Improvement'] = (mse_test_all - comparison['Test MSE']) / mse_test_all\n",
    "\n",
    "# Sort by test MSE\n",
    "comparison = comparison.sort_values('Test MSE')\n",
    "\n",
    "# Display the comparison\n",
    "pd.set_option('display.precision', 4)\n",
    "display(comparison)\n",
    "\n",
    "# Visualize the comparison\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Feature Count vs. Methods\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(comparison['Method'], comparison['Feature Count'], color='skyblue')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.title('Feature Count by Method')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Plot 2: Test MSE vs. Methods\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.bar(comparison['Method'], comparison['Test MSE'], color='lightgreen')\n",
    "plt.ylabel('Test MSE')\n",
    "plt.title('Test Error by Method')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Plot 3: Identification Rate vs. Methods\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(comparison['Method'], comparison['Identification Rate'], color='coral')\n",
    "plt.ylabel('Identification Rate')\n",
    "plt.title('Rate of Correctly Identifying True Features')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Plot 4: MSE Improvement vs. Feature Count\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(comparison['Feature Count'], comparison['MSE Improvement'], \n",
    "          s=100, c=range(len(comparison)), cmap='viridis')\n",
    "for i, method in enumerate(comparison['Method']):\n",
    "    plt.annotate(method, \n",
    "               (comparison['Feature Count'].iloc[i], comparison['MSE Improvement'].iloc[i]),\n",
    "               xytext=(5, 5), textcoords='offset points')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('MSE Improvement')\n",
    "plt.title('Performance Improvement vs. Feature Count')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-World Example: Diabetes Dataset\n",
    "\n",
    "Now let's apply our feature selection methods to a real-world dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "feature_names = diabetes.feature_names\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train_diab, X_test_diab, y_train_diab, y_test_diab = train_test_split(\n",
    "    X_diabetes, y_diabetes, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler_diab = StandardScaler()\n",
    "X_train_diab_scaled = scaler_diab.fit_transform(X_train_diab)\n",
    "X_test_diab_scaled = scaler_diab.transform(X_test_diab)\n",
    "\n",
    "print(\"Diabetes Dataset Information:\")\n",
    "print(f\"Number of samples: {X_diabetes.shape[0]}\")\n",
    "print(f\"Number of features: {X_diabetes.shape[1]}\")\n",
    "print(f\"Feature names: {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model performance\n",
    "def evaluate_diabetes_model(X_train, X_test, y_train, y_test, method_name):\n",
    "    # Train model\n",
    "    model = LinearRegression(learning_rate=0.01, max_iterations=2000, regularization=0.01)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    return {\n",
    "        'Method': method_name,\n",
    "        'Feature Count': X_train.shape[1],\n",
    "        'Training MSE': mse_train,\n",
    "        'Test MSE': mse_test,\n",
    "        'Test R²': r2_test,\n",
    "        'Model': model\n",
    "    }\n",
    "\n",
    "# Apply feature selection methods\n",
    "# 1. Correlation-based\n",
    "selected_diab_corr, _ = selector.select_k_best(X_train_diab_scaled, y_train_diab, k=5)\n",
    "\n",
    "# 2. RFE\n",
    "selected_diab_rfe, _ = selector.recursive_feature_elimination(\n",
    "    X_train_diab_scaled, y_train_diab, n_features_to_select=5)\n",
    "\n",
    "# 3. L1 regularization\n",
    "selected_diab_l1, l1_importance_diab = selector.l1_based_selection(\n",
    "    X_train_diab_scaled, y_train_diab, alpha=0.1, threshold=0.01)\n",
    "\n",
    "# Evaluate all methods\n",
    "results_diab = []\n",
    "\n",
    "# All features\n",
    "results_diab.append(evaluate_diabetes_model(\n",
    "    X_train_diab_scaled, X_test_diab_scaled, y_train_diab, y_test_diab, 'All Features'))\n",
    "\n",
    "# Correlation-based\n",
    "results_diab.append(evaluate_diabetes_model(\n",
    "    X_train_diab_scaled[:, selected_diab_corr], X_test_diab_scaled[:, selected_diab_corr], \n",
    "    y_train_diab, y_test_diab, 'Correlation-Based'))\n",
    "\n",
    "# RFE\n",
    "results_diab.append(evaluate_diabetes_model(\n",
    "    X_train_diab_scaled[:, selected_diab_rfe], X_test_diab_scaled[:, selected_diab_rfe], \n",
    "    y_train_diab, y_test_diab, 'RFE'))\n",
    "\n",
    "# L1 regularization\n",
    "results_diab.append(evaluate_diabetes_model(\n",
    "    X_train_diab_scaled[:, selected_diab_l1], X_test_diab_scaled[:, selected_diab_l1], \n",
    "    y_train_diab, y_test_diab, 'L1 Regularization'))\n",
    "\n",
    "# Create DataFrame\n",
    "df_results_diab = pd.DataFrame([\n",
    "    {k: v for k, v in result.items() if k != 'Model'} for result in results_diab\n",
    "])\n",
    "\n",
    "# Sort by test MSE\n",
    "df_results_diab = df_results_diab.sort_values('Test MSE')\n",
    "\n",
    "# Display results\n",
    "display(df_results_diab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize feature importance for the diabetes dataset and see which features were selected by each method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot weights from full model\n",
    "full_model = results_diab[0]['Model']\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(range(len(feature_names)), np.abs(full_model.weights), color='lightblue')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=90)\n",
    "plt.ylabel('Absolute Weight')\n",
    "plt.title('Feature Importance: All Features')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Plot selected features by each method\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.bar(range(len(feature_names)), np.zeros(len(feature_names)), color='white', alpha=0.0)\n",
    "for idx in selected_diab_corr:\n",
    "    plt.bar(idx, 1, color='orange')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=90)\n",
    "plt.ylabel('Selected (1=Yes)')\n",
    "plt.title('Features Selected: Correlation-Based')\n",
    "plt.ylim(0, 1.2)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(range(len(feature_names)), np.zeros(len(feature_names)), color='white', alpha=0.0)\n",
    "for idx in selected_diab_rfe:\n",
    "    plt.bar(idx, 1, color='green')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=90)\n",
    "plt.ylabel('Selected (1=Yes)')\n",
    "plt.title('Features Selected: RFE')\n",
    "plt.ylim(0, 1.2)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.bar(range(len(feature_names)), np.zeros(len(feature_names)), color='white', alpha=0.0)\n",
    "for idx in selected_diab_l1:\n",
    "    plt.bar(idx, 1, color='red')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=90)\n",
    "plt.ylabel('Selected (1=Yes)')\n",
    "plt.title('Features Selected: L1 Regularization')\n",
    "plt.ylim(0, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find common features selected by all methods\n",
    "common_features = set(selected_diab_corr)\n",
    "common_features = common_features.intersection(selected_diab_rfe)\n",
    "common_features = common_features.intersection(selected_diab_l1)\n",
    "common_feature_names = [feature_names[i] for i in common_features]\n",
    "\n",
    "print(f\"Features selected by all methods: {common_feature_names}\")\n",
    "\n",
    "# Compare model performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df_results_diab['Method'], df_results_diab['Test MSE'], color='lightgreen')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Test MSE')\n",
    "plt.title('Diabetes Dataset: Model Performance by Feature Selection Method')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Insights and Best Practices\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "1. **Benefits of Feature Selection**:\n",
    "   - Reduces model complexity without sacrificing performance\n",
    "   - Can often improve generalization by removing noise features\n",
    "   - Allows us to identify which variables are most important for predictions\n",
    "   - Different methods have different strengths in identifying important features\n",
    "\n",
    "2. **Comparison of Methods**:\n",
    "   - **Filter Methods** (Correlation-based):\n",
    "     - Simple and fast to compute\n",
    "     - Independent of the model\n",
    "     - May miss features with non-linear or interactive relationships\n",
    "   \n",
    "   - **Wrapper Methods** (RFE):\n",
    "     - Often more accurate since they use the actual model\n",
    "     - Computationally more intensive\n",
    "     - Can capture more complex relationships\n",
    "   \n",
    "   - **Embedded Methods** (L1 Regularization):\n",
    "     - Integrates feature selection with model training\n",
    "     - Can automatically determine the optimal number of features\n",
    "     - Balances model complexity with performance\n",
    "\n",
    "3. **Real-World Applications**:\n",
    "   - Different datasets may benefit from different feature selection approaches\n",
    "   - Features selected by multiple methods are often truly important\n",
    "   - The optimal number of features depends on the specific problem\n",
    "\n",
    "### Best Practices for Feature Selection\n",
    "\n",
    "1. **Always Evaluate on Validation/Test Data**\n",
    "   - Feature selection should be based on training data only\n",
    "   - Evaluate results on separate validation data\n",
    "   - Avoid data leakage between selection and evaluation\n",
    "\n",
    "2. **Consider Multiple Methods**\n",
    "   - Try different approaches and compare results\n",
    "   - Features selected by multiple methods are often important\n",
    "   - Ensemble different feature selection techniques\n",
    "\n",
    "3. **Domain Knowledge is Valuable**\n",
    "   - Incorporate subject matter expertise when possible\n",
    "   - Some features may be important despite statistical measures\n",
    "   - Context matters in interpreting feature importance\n",
    "\n",
    "4. **Balance Performance and Interpretability**\n",
    "   - Fewer features often lead to more interpretable models\n",
    "   - Consider the trade-off between simplicity and accuracy\n",
    "   - Choose the minimal set of features that provides acceptable performance\n",
    "\n",
    "Feature selection is a powerful technique in the machine learning toolkit. By carefully choosing the most relevant features, we can build models that are simpler, more accurate, and easier to understand and explain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
