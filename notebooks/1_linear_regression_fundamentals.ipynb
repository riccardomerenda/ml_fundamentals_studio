{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Fundamentals\n",
    "\n",
    "This notebook demonstrates the core concepts of linear regression implemented from scratch, including:\n",
    "- Model implementation\n",
    "- Gradient descent optimization\n",
    "- Visualizing the training process\n",
    "- Comparing our implementation with scikit-learn\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler as SklearnStandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Add the parent directory to sys.path to import our custom modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import our implementations\n",
    "from models.linear_regression import LinearRegression\n",
    "from utils.preprocessing import StandardScaler\n",
    "from utils.plotting import plot_learning_curve, plot_regression_results\n",
    "from datasets.data_utils import generate_synthetic_data\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "We'll start by generating a synthetic dataset with a known linear relationship, plus some noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data using scikit-learn's make_regression\n",
    "X, y, coef = make_regression(n_samples=100, n_features=1, noise=10, \n",
    "                             coef=True, random_state=42, bias=5.0)\n",
    "\n",
    "# Alternative: use our custom function\n",
    "# X, y = generate_synthetic_data(n_samples=100, n_features=1, noise=10, function_type='linear', random_state=42)\n",
    "# X is returned as a 2D array, let's flatten for easier plotting\n",
    "# X = X.flatten()\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training data')\n",
    "plt.scatter(X_test, y_test, color='red', label='Testing data')\n",
    "plt.title('Synthetic Regression Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"True coefficient: {coef:.4f}\")\n",
    "print(f\"Number of training samples: {X_train.shape[0]}\")\n",
    "print(f\"Number of testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression from Scratch\n",
    "\n",
    "Next, we'll use our custom implementation of linear regression to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train our linear regression model\n",
    "model = LinearRegression(learning_rate=0.01, max_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Print the learned weights\n",
    "print(f\"Learned weight (w): {model.weights[0]:.4f}\")\n",
    "print(f\"Learned bias (b): {model.bias:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(f\"Mean Squared Error on test set: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Learning Process\n",
    "\n",
    "Let's visualize how the cost decreased during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "fig = plot_learning_curve(model, title=\"Linear Regression Learning Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the learned model against the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the regression results\n",
    "fig = plot_regression_results(model, X_train, y_train, X_test, y_test, \n",
    "                             title=\"Linear Regression Results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing Learning Rates\n",
    "\n",
    "Let's compare the effect of different learning rates on convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    model = LinearRegression(learning_rate=lr, max_iterations=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    plt.plot(range(len(model.cost_history)), model.cost_history, \n",
    "             label=f'Learning rate = {lr}')\n",
    "\n",
    "plt.title('Effect of Learning Rate on Convergence')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multiple Linear Regression\n",
    "\n",
    "Now let's try with multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with multiple features\n",
    "X_multi, y_multi = make_regression(n_samples=100, n_features=5, noise=10, \n",
    "                                  random_state=42, bias=5.0)\n",
    "\n",
    "# Split the data\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model_multi = LinearRegression(learning_rate=0.01, max_iterations=2000)\n",
    "model_multi.fit(X_train_multi, y_train_multi)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_multi = model_multi.predict(X_test_multi)\n",
    "mse_multi = mean_squared_error(y_test_multi, y_pred_multi)\n",
    "\n",
    "print(\"Multiple Linear Regression Results:\")\n",
    "print(f\"Number of features: {X_multi.shape[1]}\")\n",
    "print(f\"Learned weights: {model_multi.weights}\")\n",
    "print(f\"Learned bias: {model_multi.bias:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse_multi:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison with scikit-learn\n",
    "\n",
    "Let's compare our implementation with scikit-learn's LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a scikit-learn model on the original data\n",
    "sklearn_model = SklearnLinearRegression()\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "sklearn_y_pred = sklearn_model.predict(X_test)\n",
    "sklearn_mse = mean_squared_error(y_test, sklearn_y_pred)\n",
    "\n",
    "# Compare results\n",
    "print(\"Comparison with scikit-learn:\")\n",
    "print(f\"Our model weight: {model.weights[0]:.4f}, bias: {model.bias:.4f}\")\n",
    "print(f\"scikit-learn weight: {sklearn_model.coef_[0]:.4f}, bias: {sklearn_model.intercept_:.4f}\")\n",
    "print(f\"Our model test MSE: {mse:.4f}\")\n",
    "print(f\"scikit-learn test MSE: {sklearn_mse:.4f}\")\n",
    "\n",
    "# Plot both models together\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', alpha=0.5, label='Training data')\n",
    "plt.scatter(X_test, y_test, color='green', alpha=0.5, label='Test data')\n",
    "\n",
    "# Create line for predictions\n",
    "X_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "y_line_our = model.predict(X_line)\n",
    "y_line_sklearn = sklearn_model.predict(X_line)\n",
    "\n",
    "plt.plot(X_line, y_line_our, 'r-', label='Our model', linewidth=2)\n",
    "plt.plot(X_line, y_line_sklearn, 'g--', label='scikit-learn model', linewidth=2)\n",
    "\n",
    "plt.title('Comparison with scikit-learn')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding the Gradient Descent Algorithm\n",
    "\n",
    "Let's visualize how gradient descent works for a simple case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple function to illustrate gradient descent\n",
    "def f(w, b):\n",
    "    \"\"\"A simple quadratic function: f(w, b) = w^2 + b^2\"\"\"\n",
    "    return w**2 + b**2\n",
    "\n",
    "def gradient(w, b):\n",
    "    \"\"\"Gradient of f: [df/dw, df/db] = [2w, 2b]\"\"\"\n",
    "    return np.array([2*w, 2*b])\n",
    "\n",
    "# Initial point\n",
    "w, b = 4.0, -3.0\n",
    "learning_rate = 0.1\n",
    "iterations = 20\n",
    "\n",
    "# Lists to store the trajectory\n",
    "ws = [w]\n",
    "bs = [b]\n",
    "costs = [f(w, b)]\n",
    "\n",
    "# Gradient descent\n",
    "for i in range(iterations):\n",
    "    grad = gradient(w, b)\n",
    "    w -= learning_rate * grad[0]\n",
    "    b -= learning_rate * grad[1]\n",
    "    \n",
    "    ws.append(w)\n",
    "    bs.append(b)\n",
    "    costs.append(f(w, b))\n",
    "    \n",
    "# Create a meshgrid for visualization\n",
    "w_grid = np.linspace(-5, 5, 100)\n",
    "b_grid = np.linspace(-5, 5, 100)\n",
    "W, B = np.meshgrid(w_grid, b_grid)\n",
    "Z = f(W, B)\n",
    "\n",
    "# Plot the contour and the trajectory\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot contour\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.contourf(W, B, Z, 50, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(label='f(w, b)')\n",
    "plt.plot(ws, bs, 'r-o', linewidth=2, markersize=5)\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('b')\n",
    "plt.title('Gradient Descent Path')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot cost vs iteration\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(iterations + 1), costs, 'b-o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs. Iteration')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial point: w={ws[0]}, b={bs[0]}, cost={costs[0]:.4f}\")\n",
    "print(f\"Final point: w={ws[-1]:.4f}, b={bs[-1]:.4f}, cost={costs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "In this notebook, we've explored:\n",
    "\n",
    "1. How to implement linear regression from scratch using gradient descent\n",
    "2. The effect of learning rate on convergence\n",
    "3. How to visualize the learning process\n",
    "4. Multiple linear regression with several features\n",
    "5. Comparison with scikit-learn's implementation\n",
    "\n",
    "Key takeaways:\n",
    "- Gradient descent is an iterative optimization algorithm that minimizes the cost function\n",
    "- The learning rate is a critical hyperparameter that affects convergence speed and stability\n",
    "- Our implementation produces results very similar to scikit-learn's implementation\n",
    "- Linear regression can be extended to multiple features using the same principles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
