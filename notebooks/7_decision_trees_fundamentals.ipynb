{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db92512",
   "metadata": {},
   "source": [
    "# Decision Trees Fundamentals\n",
    "\n",
    "In this notebook, we'll explore Decision Trees, a versatile machine learning algorithm that can be used for both classification and regression tasks. Decision trees make predictions by learning simple decision rules inferred from the data features.\n",
    "\n",
    "We'll cover:\n",
    "1. Understanding Decision Tree structure and components\n",
    "2. Implementing a Decision Tree from scratch\n",
    "3. Training and making predictions\n",
    "4. Visualizing decision boundaries\n",
    "5. Comparing with scikit-learn's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path to import custom modules\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('.'))))\n",
    "\n",
    "# Import custom modules\n",
    "from models.decision_tree import DecisionTree\n",
    "from utils.data_generator import generate_nonlinear_data\n",
    "from utils.plotting import plot_decision_boundary\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2e82f",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "We'll create a synthetic dataset with a non-linear decision boundary to demonstrate the capabilities of decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2fa3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = generate_nonlinear_data(n_samples=1000, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Synthetic Classification Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b8c68b",
   "metadata": {},
   "source": [
    "## 2. Train Our Custom Decision Tree\n",
    "\n",
    "Now we'll train our custom decision tree implementation on the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train our custom decision tree\n",
    "dt = DecisionTree(max_depth=5)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee955af1",
   "metadata": {},
   "source": [
    "## 3. Visualize Decision Boundary\n",
    "\n",
    "Let's visualize the decision boundary learned by our custom decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471acd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary for our custom implementation\n",
    "plot_decision_boundary(X_test, y_test, dt, title='Custom Decision Tree Decision Boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c7807",
   "metadata": {},
   "source": [
    "## 4. Compare with Scikit-learn\n",
    "\n",
    "Let's compare our implementation with scikit-learn's decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2fe3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train scikit-learn's decision tree\n",
    "sklearn_dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "sklearn_dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "sklearn_y_pred = sklearn_dt.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "sklearn_accuracy = np.mean(sklearn_y_pred == y_test)\n",
    "print(f'Scikit-learn Decision Tree accuracy: {sklearn_accuracy:.4f}')\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(X_test, y_test, sklearn_dt, title='Scikit-learn Decision Tree Decision Boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16c53d",
   "metadata": {},
   "source": [
    "## 5. Effect of Tree Depth\n",
    "\n",
    "Let's explore how the maximum depth of the tree affects its performance and decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [1, 3, 5, 10]\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, depth in enumerate(depths, 1):\n",
    "    # Train decision tree with different max_depth\n",
    "    dt = DecisionTree(max_depth=depth)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.subplot(2, 2, i)\n",
    "    plot_decision_boundary(X_test, y_test, dt, \n",
    "                          title=f'Decision Boundary (max_depth={depth})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e00860",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored decision trees by:\n",
    "1. Implementing a decision tree classifier from scratch\n",
    "2. Training it on synthetic data\n",
    "3. Visualizing its decision boundaries\n",
    "4. Comparing it with scikit-learn's implementation\n",
    "5. Analyzing the effect of tree depth on model performance\n",
    "\n",
    "Key takeaways:\n",
    "- Decision trees create rectangular decision boundaries\n",
    "- Increasing tree depth leads to more complex decision boundaries\n",
    "- Too much depth can lead to overfitting\n",
    "- Our implementation performs similarly to scikit-learn's "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
