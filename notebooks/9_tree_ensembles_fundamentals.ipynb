{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb139d3",
   "metadata": {},
   "source": [
    "# Tree Ensembles Fundamentals\n",
    "\n",
    "In this notebook, we'll explore Tree Ensembles, which combine multiple decision trees to create more powerful and robust models. We'll focus on Random Forests and Gradient Boosting, two popular ensemble methods.\n",
    "\n",
    "We'll cover:\n",
    "1. Understanding ensemble methods and their advantages\n",
    "2. Implementing Random Forest from scratch\n",
    "3. Training and making predictions\n",
    "4. Visualizing ensemble decision boundaries\n",
    "5. Comparing with scikit-learn's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "469626c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path to import custom modules\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Import custom modules\n",
    "from models.random_forest import RandomForest\n",
    "from utils.data_generator import generate_nonlinear_data\n",
    "from utils.plotting import plot_decision_boundary\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e3c8d",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "We'll create a synthetic dataset with a complex decision boundary to demonstrate the power of tree ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e9734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = generate_nonlinear_data(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Synthetic Classification Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f443f",
   "metadata": {},
   "source": [
    "## 2. Train Our Custom Random Forest\n",
    "\n",
    "Now we'll train our custom random forest implementation on the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d75043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train our custom random forest\n",
    "rf = RandomForest(n_trees=10, max_depth=5, min_samples_split=2)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24474b9e",
   "metadata": {},
   "source": [
    "## 3. Visualize Decision Boundary\n",
    "\n",
    "Let's visualize the decision boundary learned by our custom random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary for our custom implementation\n",
    "plot_decision_boundary(X_test, y_test, rf, title='Custom Random Forest Decision Boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c226c",
   "metadata": {},
   "source": [
    "## 4. Compare with Scikit-learn\n",
    "\n",
    "Let's compare our implementation with scikit-learn's random forest and gradient boosting classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee99fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train scikit-learn's random forest\n",
    "sklearn_rf = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=42)\n",
    "sklearn_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "sklearn_rf_pred = sklearn_rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "sklearn_rf_accuracy = np.mean(sklearn_rf_pred == y_test)\n",
    "print(f'Scikit-learn Random Forest accuracy: {sklearn_rf_accuracy:.4f}')\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(X_test, y_test, sklearn_rf, title='Scikit-learn Random Forest Decision Boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train scikit-learn's gradient boosting\n",
    "sklearn_gb = GradientBoostingClassifier(n_estimators=10, max_depth=5, random_state=42)\n",
    "sklearn_gb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "sklearn_gb_pred = sklearn_gb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "sklearn_gb_accuracy = np.mean(sklearn_gb_pred == y_test)\n",
    "print(f'Scikit-learn Gradient Boosting accuracy: {sklearn_gb_accuracy:.4f}')\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(X_test, y_test, sklearn_gb, title='Scikit-learn Gradient Boosting Decision Boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226ad01a",
   "metadata": {},
   "source": [
    "## 5. Effect of Ensemble Size\n",
    "\n",
    "Let's explore how the number of trees in the ensemble affects the model's performance and decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90db42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees_list = [1, 5, 10, 20]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()  # Flatten the 2x2 array of axes\n",
    "\n",
    "for i, n_trees in enumerate(n_trees_list):\n",
    "    # Train random forest with different number of trees\n",
    "    rf = RandomForest(n_trees=n_trees, max_depth=5, min_samples_split=2)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Create mesh grid for decision boundary\n",
    "    x_min, x_max = X_test[:, 0].min() - 0.5, X_test[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_test[:, 1].min() - 0.5, X_test[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                        np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    # Make predictions\n",
    "    Z = rf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot on the appropriate subplot\n",
    "    axes[i].contourf(xx, yy, Z, alpha=0.4)\n",
    "    axes[i].scatter(X_test[:, 0], X_test[:, 1], c=y_test, alpha=0.8)\n",
    "    axes[i].set_title(f'Decision Boundary (n_trees={n_trees})')\n",
    "    axes[i].set_xlabel('Feature 1')\n",
    "    axes[i].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f408e37",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored tree ensembles by:\n",
    "1. Implementing a random forest from scratch\n",
    "2. Training it on synthetic data\n",
    "3. Visualizing its decision boundaries\n",
    "4. Comparing with scikit-learn's implementations\n",
    "5. Analyzing the effect of ensemble size on model performance\n",
    "\n",
    "Key takeaways:\n",
    "- Tree ensembles create more robust and accurate models\n",
    "- Random forests reduce overfitting through averaging\n",
    "- Gradient boosting builds strong learners sequentially\n",
    "- Increasing the number of trees generally improves performance "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
